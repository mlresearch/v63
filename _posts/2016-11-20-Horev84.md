---
title: Geometry-aware stationary subspace analysis
abstract: In many real-world applications data exhibits non-stationarity, i.e., its
  distribution changes over time.  One approach to handling non-stationarity is to
  remove or minimize it before attempting to analyze the data.  In the context of
  brain computer interface (BCI) data analysis this is sometimes achieved using stationary
  subspace analysis (SSA).  The classic SSA method finds a matrix that projects the
  data onto a stationary subspace by optimizing a cost function based on a matrix
  divergence.  In this work we present an alternative method for SSA based on a symmetrized
  version of this matrix divergence.  We show that this frames the problem in terms
  of distances between symmetric positive definite (SPD) matrices, suggesting a geometric
  interpretation of the problem.  Stemming from this geometric viewpoint, we introduce
  and analyze a method which utilizes the geometry of the SPD matrix manifold and
  the invariance properties of its metrics.  Most notably we show that these invariances
  alleviate the need to whiten the input matrices, a common step in many SSA methods
  which often introduces error.  We demonstrate the usefulness of our technique in
  experiments on both synthetic and real-world data.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Horev84
month: 0
firstpage: 430
lastpage: 444
page: 430-444
sections: 
author:
- given: Inbal
  family: Horev
- given: Florian
  family: Yger
- given: Masashi
  family: Sugiyama
date: 2016-11-20
address: The University of Waikato, Hamilton, New Zealand
publisher: PMLR
container-title: Proceedings of The 8th Asian Conference on Machine Learning
volume: '63'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 11
  - 20
pdf: http://proceedings.mlr.press/v63/Horev84.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
