---
title: Deep Gate Recurrent Neural Network
abstract: This paper explores the possibility of using multiplicative gates to build
  two recurrent neural network structures. These two structures are called Deep Simple
  Gated Unit (DSGU) and Simple Gated Unit (SGU), which are structures for learning
  long-term dependencies. Compared to traditional Long Short-Term Memory (LSTM) and
  Gated Recurrent Unit (GRU), both structures require fewer parameters and less computation
  time in sequence classification tasks. Unlike GRU and LSTM, which require more than
  one gate to control information flow in the network, SGU and DSGU only use one multiplicative
  gate to control the flow of information. We show that this difference can accelerate
  the learning speed in tasks that require long dependency information. We also show
  that DSGU is more numerically stable than SGU. In addition, we also propose a standard
  way of representing the inner structure of RNN called RNN Conventional Graph (RCG),
  which helps to analyze the relationship between input units and hidden units of
  RNN.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gao30
month: 0
tex_title: Deep Gate Recurrent Neural Network
firstpage: 350
lastpage: 365
page: 350-365
order: 350
cycles: false
author:
- given: Yuan
  family: Gao
- given: Dorota
  family: Glowacka
date: 2016-11-20
address: The University of Waikato, Hamilton, New Zealand
publisher: PMLR
container-title: Proceedings of The 8th Asian Conference on Machine Learning
volume: '63'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 11
  - 20
pdf: http://proceedings.mlr.press/v63/gao30.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
