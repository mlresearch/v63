---
title: Improving Distributed Word Representation and Topic Model by Word-Topic Mixture
  Model
abstract: We propose a Word-Topic Mixture(WTM) model to improve word representation
  and topic model simultaneously. Firstly, it introduces the initial external word
  embeddings into the Topical Word Embeddings(TWE) model based on Latent Dirichlet
  Allocation(LDA) model to learn word embeddings and topic vectors. Then the results
  learned from TWE are integrated in the LDA by defining the probability distribution
  of topic vectors-word embeddings according to the idea of latent feature model with
  LDA (LFLDA), meanwhile minimizing the KL divergence of the new topic-word distribution
  function and the original one. The experimental results prove that the WTM model
  performs better on word representation and topic detection compared with some state-of-the-art
  models.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Fu60
month: 0
tex_title: Improving Distributed Word Representation and Topic Model by Word-Topic
  Mixture Model
firstpage: 190
lastpage: 205
page: 190-205
order: 190
cycles: false
author:
- given: Xianghua
  family: Fu
- given: Ting
  family: Wang
- given: Jing
  family: Li
- given: Chong
  family: Yu
- given: Wangwang
  family: Liu
date: 2016-11-20
address: The University of Waikato, Hamilton, New Zealand
publisher: PMLR
container-title: Proceedings of The 8th Asian Conference on Machine Learning
volume: '63'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 11
  - 20
pdf: http://proceedings.mlr.press/v63/Fu60.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
