---
title: Multitask Principal Component Analysis
abstract: Principal Component Analysis (PCA) is a canonical and well-studied tool
  for dimensionality reduction. However, when few data are available, the poor quality
  of the covariance estimator at its core may compromise its performance. We leverage
  this issue by casting the PCA into a multitask framework, and doing so, we show
  how to solve simultaneously several related PCA problems. Hence, we propose a novel
  formulation of the PCA problem relying on a novel regularization. This regularization
  is based on a distance between subspaces, and the whole problem is solved as an
  optimization problem over a Riemannian manifold. We experimentally demonstrate the
  usefulness of our approach as pre-processing for EEG signals.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yamane65
month: 0
tex_title: Multitask Principal Component Analysis
firstpage: 302
lastpage: 317
page: 302-317
order: 302
cycles: false
author:
- given: Ikko
  family: Yamane
- given: Florian
  family: Yger
- given: Maxime
  family: Berar
- given: Masashi
  family: Sugiyama
date: 2016-11-20
address: The University of Waikato, Hamilton, New Zealand
publisher: PMLR
container-title: Proceedings of The 8th Asian Conference on Machine Learning
volume: '63'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 11
  - 20
pdf: http://proceedings.mlr.press/v63/yamane65.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
