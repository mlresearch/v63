<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Learnability of Non-I.I.D. | ACML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Learnability of Non-I.I.D.">

  <meta name="citation_author" content="Gao, Wei">

  <meta name="citation_author" content="Niu, Xin-Yi">

  <meta name="citation_author" content="Zhou, Zhi-Hua">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 8th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="158">
<meta name="citation_lastpage" content="173">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v63/Gao09.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Learnability of Non-I.I.D.</h1>

	<div id="authors">
	
		Wei Gao,
	
		Xin-Yi Niu,
	
		Zhi-Hua Zhou
	<br />
	</div>
	<div id="info">
		Proceedings of The 8th Asian Conference on Machine Learning,
		pp. 158â€“173, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Learnability has always been one of the most central problems in learning theory. Most previous studies on this issue were based on the assumption that the samples are drawn independently and identically according to an underlying (unknown) distribution. The i.i.d. assumption, however, does not hold in many real applications. In this paper, we study the learnability of problems where the samples are drawn from empirical process of stationary <span class="math">\(\beta\)</span>-mixing sequence, which has been a widely-used assumption implying a dependence weaken over time in training samples. By utilizing the independent blocks technique, we provide a sufficient and necessary condition for learnability, that is, average stability is equivalent to learnability with AERM (Asymptotic Empirical Risk Minimization) in the non-i.i.d. learning setting. In addition, we also discuss the generalization error when the test variable is dependent on the training sample.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Gao09.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
