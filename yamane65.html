<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Multitask Principal Component Analysis | ACML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Multitask Principal Component Analysis">

  <meta name="citation_author" content="Yamane, Ikko">

  <meta name="citation_author" content="Yger, Florian">

  <meta name="citation_author" content="Berar, Maxime">

  <meta name="citation_author" content="Sugiyama, Masashi">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 8th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="302">
<meta name="citation_lastpage" content="317">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v63/yamane65.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Multitask Principal Component Analysis</h1>

	<div id="authors">
	
		Ikko Yamane,
	
		Florian Yger,
	
		Maxime Berar,
	
		Masashi Sugiyama
	<br />
	</div>
	<div id="info">
		Proceedings of The 8th Asian Conference on Machine Learning,
		pp. 302â€“317, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Principal Component Analysis (PCA) is a canonical and well-studied tool for dimensionality reduction. However, when few data are available, the poor quality of the covariance estimator at its core may compromise its performance. We leverage this issue by casting the PCA into a multitask framework, and doing so, we show how to solve simultaneously several related PCA problems. Hence, we propose a novel formulation of the PCA problem relying on a novel regularization. This regularization is based on a distance between subspaces, and the whole problem is solved as an optimization problem over a Riemannian manifold. We experimentally demonstrate the usefulness of our approach as pre-processing for EEG signals.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="yamane65.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
