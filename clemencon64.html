<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Learning from Survey Training Samples: Rate Bounds for Horvitz-Thompson Risk Minimizers | ACML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Learning from Survey Training Samples: Rate Bounds for Horvitz-Thompson Risk Minimizers">

  <meta name="citation_author" content="Clemencon, Stephan">

  <meta name="citation_author" content="Bertail, Patrice">

  <meta name="citation_author" content="Papa, Guillaume">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 8th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="142">
<meta name="citation_lastpage" content="157">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v63/clemencon64.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Learning from Survey Training Samples: Rate Bounds for Horvitz-Thompson Risk Minimizers</h1>

	<div id="authors">
	
		Stephan Clemencon,
	
		Patrice Bertail,
	
		Guillaume Papa
	<br />
	</div>
	<div id="info">
		Proceedings of The 8th Asian Conference on Machine Learning,
		pp. 142–157, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		The generalization ability of minimizers of the empirical risk in the context of binary classification has been investigated under a wide variety of complexity assumptions for the collection of classifiers over which optimization is performed. In contrast, the vast majority of the works dedicated to this issue stipulate that the training dataset used to compute the empirical risk functional is composed of i.i.d. observations and involve sharp control of uniform deviation of i.i.d. averages from their expectation. Beyond the cases where training data are drawn uniformly without replacement among a large i.i.d. sample or modelled as a realization of a weakly dependent sequence of r.v.’s, statistical guarantees when the data used to train a classifier are drawn by means of a more general sampling/survey scheme and exhibit a complex dependence structure have not been documented in the literature yet. It is the main purpose of this paper to show that the theory of empirical risk minimization can   be extended to situations where statistical learning is based on survey samples and knowledge of the related (first order) inclusion probabilities. Precisely, we prove that minimizing a (possibly biased) weighted version of the empirical risk, refered to as the (approximate) Horvitz-Thompson risk (HT risk), over a class of controlled complexity lead to a rate for the excess risk of the order <span class="math">\(O_{\mathbb{P}}((\kappa_N (\log N)/n)^{1/2})\)</span> with <span class="math">\(\kappa_N=(n/N)/\min_{i\leq N}\pi_i\)</span>, when data are sampled by means of a rejective scheme of (deterministic) size <span class="math">\(n\)</span> within a statistical population of cardinality <span class="math">\(N\geq n\)</span>, a generalization of basic <span><em>sampling without replacement</em></span> with unequal probability weights <span class="math">\(\pi_i &gt; 0\)</span>. Extension to other sampling schemes are then established by a coupling argument. Beyond theoretical results, numerical experiments are displayed in order to show the relevance of HT risk minimization and that ignoring the sampling scheme used to generate the training dataset may completely jeopardize the learning procedure.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="clemencon64.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
