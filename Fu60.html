<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Improving Distributed Word Representation and Topic Model by Word-Topic Mixture Model | ACML 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Improving Distributed Word Representation and Topic Model by Word-Topic Mixture Model">

  <meta name="citation_author" content="Fu, Xianghua">

  <meta name="citation_author" content="Wang, Ting">

  <meta name="citation_author" content="Li, Jing">

  <meta name="citation_author" content="Yu, Chong">

  <meta name="citation_author" content="Liu, Wangwang">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 8th Asian Conference on Machine Learning">
<meta name="citation_firstpage" content="190">
<meta name="citation_lastpage" content="205">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v63/Fu60.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Improving Distributed Word Representation and Topic Model by Word-Topic Mixture Model</h1>

	<div id="authors">
	
		Xianghua Fu,
	
		Ting Wang,
	
		Jing Li,
	
		Chong Yu,
	
		Wangwang Liu
	<br />
	</div>
	<div id="info">
		Proceedings of The 8th Asian Conference on Machine Learning,
		pp. 190â€“205, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We propose a Word-Topic Mixture(WTM) model to improve word representation and topic model simultaneously. Firstly, it introduces the initial external word embeddings into the Topical Word Embeddings(TWE) model based on Latent Dirichlet Allocation(LDA) model to learn word embeddings and topic vectors. Then the results learned from TWE are integrated in the LDA by defining the probability distribution of topic vectors-word embeddings according to the idea of latent feature model with LDA (LFLDA), meanwhile minimizing the KL divergence of the new topic-word distribution function and the original one. The experimental results prove that the WTM model performs better on word representation and topic detection compared with some state-of-the-art models.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Fu60.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
